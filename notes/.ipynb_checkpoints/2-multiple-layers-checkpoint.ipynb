{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw new expressions for what occurs with a sigmoid neuron.  In general we can think of a neuron as having two components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multiple dendrites which each receive a signal from an input \n",
    "* And a cell body that gets turned on based on a combination of the inputs from the dendrites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./neuron-math.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above, $x_o$ is the input from the outside, which combined with a corresponding weight feeds into the cell body.  The cell body activates or does not activate based on the combination of these inputs.  And importantly, it mimics the pattern of receiving a linear function from the inputs and a non-linear function -- a binary turn on or off -- as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that if we have inputs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And corresponding weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our linear function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \n",
    "\\end{bmatrix} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we simply wrap this output in a sigmoid function, so we can write our entire sigmoid neuron as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S(x) = \\sigma(x \\cdot w + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complicated neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's go back to our example of a neuron determining whether food it tastes is sweet or not.  This time, let's say that the observation of the food involves the following:\n",
    "\n",
    "* sweet taste 2\n",
    "* sweet smell 4\n",
    "* salty taste 3\n",
    "* salty smell 1\n",
    "\n",
    "And we capture this data in the vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 4, 3, 1])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even though, this neuron is in charge of determining if something is sweet, it still observes all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sweet = np.array([1, 3, 0, -.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the more it detects a salty smell, it is less likely to find something sweet.  After all if something is smells salty, it could overpower a determination of sweetness.  And note that in making it's determination, it does not consider a saltiness taste at all(as indicated by the 0).\n",
    "\n",
    "Let's set the bias at $-12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sweet = -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(w_sweet) + b_sweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175744761936437"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_sweet) + b_neuron_sweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed this data into a second neuron, who makes a different decision - whether or not something is salty.  Remember that our observation looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 4, 3, 1])\n",
    "x\n",
    "# sweet taste 2, sweet smell 4, salty taste 3, salty smell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_salty = np.array([0, -.5, 3, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice this time around, the sweetness weights are either 0 or negative - as this neuron is about determining if something is salty.  The neuron has it's own bias for saltiness, which is $-7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_salty = -8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_salty) + b_salty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking with multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple points to take from the above discussion. \n",
    "\n",
    "#### 1. Different weights same input\n",
    "\n",
    "The first point, it is the same vector $x$ that is an input to each neuron, and that these neurons simply weigh these inputs differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f\\_sweet(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "\\end{bmatrix} - 12$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f\\_salty(x) = \\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} - 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175744761936437"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x.dot(w_salty) + b_salty)\n",
    "\n",
    "sigmoid(x.dot(w_sweet) + b_sweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so if you look at a neural network diagram, you will see a diagram illustrating this point that each input goes to each neuron in that first layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./first-layer.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The diagram above illustrates a neural network where each observation has four features, and each feature is an input to each of the four neurons in the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. We can make it brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second point is that the only thing different from neuron to neuron is the weight vector and the bias.  Let's leave aside the biases for a moment, leaving us with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f\\_sweet(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "\\end{bmatrix} = 2*1 + 4*3 + 3*0 + 1*-.5 = 13.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f\\_salty(x) = \\begin{bmatrix}\n",
    "2 & 4 & 3 & 1 \\\\\n",
    "\\end{bmatrix}\\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-.5 \\\\\n",
    "3 \\\\\n",
    "1.5 \\\\\n",
    "\\end{bmatrix} = 2*0 + 4*-.5 + 3*3 + 1*1.5 = 8.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe the following: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & w_1 & - & -  \\\\\n",
    "- & w_2 & - & - \\end{bmatrix}\n",
    "\\cdot \\begin{bmatrix}\n",
    "| \\\\ x\\\\ | \\\\ | \n",
    "\\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_1 \\\\ x \\cdot w_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or applied to our example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{bmatrix}\n",
    "1 & 3 & 0 & -.5 \\\\\n",
    "0 & -.5 & 3 & 1.5  \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "2 \\\\ 4 \\\\ 3 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}\n",
    "13.5 \\\\ 8.5 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  3. ,  0. , -0.5],\n",
       "       [ 0. , -0.5,  3. ,  1.5]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.stack([w_sweet, w_salty])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.5,  8.5])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = W.dot(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point we have just summarized the weights of multiple neurons, however we still have not included the biases.  To complete our linear component, we need to add the bias of $-12$ to $13.5$ and the bias of $-8$ to $8.5$.  We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 0.5])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dot(x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "- & w_1 & - & -  \\\\\n",
    "- & w_2 & - & - \\end{bmatrix}\n",
    "\\cdot \\begin{bmatrix}\n",
    "| \\\\ x\\\\ | \\\\ | \n",
    "\\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}  = \\begin{bmatrix}\n",
    "x \\cdot w_1 \\\\ x \\cdot w_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can summarize the linear component of a layer of neurons as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given a matrix $W$,  Where each row of W represents the weights of a different neuron, and a vector $b$ where each entry of $b$ represents the corresponding bias of a neuron, we can calculate the outputs each of our sigmoid neurons in a layer as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81757448, 0.62245933])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(W.dot(x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(W\\cdot x + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each of our observed inputs go into each logistic regression function.  What's special about this, is that we do not have to specify what each logistic regression function is trying to predict.\n",
    "\n",
    "Instead, we have the logistic regression functions each output \"something useful\", which can be used to predict our target -- like whether a review is positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./log-to-prediction.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice that the inputs to our hypothesis function aren't features, but rather the outputs of the intermediate layer.  So the backpropagation algorithm asks the middle layers to find useful signals from the underlying data, such tht it helps the final classifier make a prediction.  These middle layers are called the **hidden layers** of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, data scientists can simply introduce more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./multilayer.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that when we think about how we would model an individual neuron, it lines up to our logistic regression function.  A logistic regression function has inputs which feed into a linear model, which then is passed to a non-linear function (our activation function).  With a neuron, we have various inputs which each enter the cell body through a dendrite.  We can think of each dendrite as amplifying or softening the input it receives.  Then the output of the cell body is determined by combination of these inputs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
