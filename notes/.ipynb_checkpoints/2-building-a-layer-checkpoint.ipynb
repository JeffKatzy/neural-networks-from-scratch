{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how to represent our neural network with matrices, now let's see how matrix multiplication comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our general formula for linear regression is the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review how to represent the linear component of a single neuron, that is a linear regression function with matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "X = torch.randint(-10, 10, (1, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above initializes an matrix that has one observation, and four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we something to represent our linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = 3x_1 + 2x_2 + 10x_3 + 6x_4 + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our function above with our matrix, X, above with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  2, 10,  6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1 = torch.tensor([3, 2, 10, 6])\n",
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll worry about representing our value of $b$, here 10, later on.  Let's use `view` to allocate a separate row for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 2],\n",
       "        [10],\n",
       "        [ 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1 = w_1.view(4, 1)\n",
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $x$ is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now for linear regression, our goal is to multiply the rows of X, by our column of weights.  This way we will have something like the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-6*3 + 9*2 + 3*10 + -10*6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we accomplish this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\cdot w_1 = \\begin{bmatrix}\n",
    " -6& 9 &3  &-10\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} \n",
    "3 \\\\\n",
    " 2\n",
    "\\\\ \n",
    "10\n",
    "\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-30]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X@w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can add `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 10\n",
    "X@w_1 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that would be the hypothesis ofÂ our single linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once again, we'll work with only one observation, except that this time we'll see what it's like to work with two linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's start with our observation with four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to feed our observation through two neurons, we can add a second row to our weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here was our first linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = 3x_1 + 2x_2 + 10x_3 + 6x_4 + 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 2],\n",
       "        [10],\n",
       "        [ 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is our second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g(x) = 4x_1 + 5x_2 + 6x_3 + x_4 + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2 = torch.tensor([4, 5, 6, 1]).view(4, 1)\n",
    "w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's add them together to build a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4],\n",
       "        [ 2,  5],\n",
       "        [10,  6],\n",
       "        [ 6,  1]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.stack((w_1, w_2), 2).view(4, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4],\n",
       "        [ 2,  5],\n",
       "        [10,  6],\n",
       "        [ 6,  1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = W.squeeze()\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we get each of these linear functions to make their own hypothesis?\n",
    "\n",
    "We can do simple matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\cdot W = \\begin{bmatrix}\n",
    " -6& 9 &3  &-10\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "3 &4 \\\\ \n",
    " 2&5 \\\\ \n",
    " 10& 6\\\\ \n",
    "6 & 1\n",
    "\\end{bmatrix} = \\begin{bmatrix} -30 \\\\ -29 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this in Python isn't so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-30,  29]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = X@W \n",
    "l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add in our bias terms.\n",
    "\n",
    "$b = \\begin{bmatrix} 10 \\\\ 2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([10, 2])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the dimensions of our bias vector is the same as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  2]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.view(1, 2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20,  31]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great.  We just made two predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we saw how we can use a weight matrix to make our predictions with some matrix multiplication, let's move this into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4],\n",
       "        [ 2,  5],\n",
       "        [10,  6],\n",
       "        [ 6,  1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_linear(X, W, b):\n",
    "    return (X@W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-25,  41]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_linear(X, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we made just built a first layer of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activating our neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After passing our data through two linear functions, the next thing to do is to apply the outputs of our linear functions to a non-linear component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we can use the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(layer):\n",
    "    return 1/(1 + np.exp(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 2.5437e-13]], dtype=torch.float64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = sigmoid(l1)\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this lesson we saw that we can apply multiple linear functions to an observation simultaneously with a weight matrix.\n",
    "\n",
    "$x \\cdot W = \\begin{bmatrix}\n",
    " -6& 9 &3  &-10\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "3 &4 \\\\ \n",
    " 2&5 \\\\ \n",
    " 10& 6\\\\ \n",
    "6 & 1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "10 \\\\ \n",
    " 2 \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix} -20 \\\\ 31 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we saw how we can create a layer of a neural network with a linear component followed by a non-linearity.  We created the linear component with a weight matrix which we multiplied to our observation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\cdot W = \\begin{bmatrix}\n",
    " -6& 9 &3  &-10\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "3 &4 \\\\ \n",
    " 2&5 \\\\ \n",
    " 10& 6\\\\ \n",
    "6 & 1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "10 \\\\ \n",
    " 2 \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix} -20 \\\\ 31 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6,   9,   3, -10]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "X = torch.randint(-10, 10, (1, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = torch.tensor([3, 2, 10, 6]).view(4, 1)\n",
    "w_2 = torch.tensor([4, 5, 6, 1]).view(4, 1)\n",
    "W = torch.stack((w_1, w_2), 2).view(4, 2)\n",
    "W\n",
    "\n",
    "b = torch.tensor([10, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_linear(X, W, b):\n",
    "    return (X@W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-20,  31]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_linear(X, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(\\begin{bmatrix} -20 \\\\ 31 \\end{bmatrix}) = \\begin{bmatrix} \\sigma(-20) \\\\ \\sigma(31) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def execute_sigmoid(layer):\n",
    "    return 1/(1 + np.exp(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_layer(X, W, b):\n",
    "    l1 = execute_layer(X, W, b)\n",
    "    l2 = execute_sigmoid(l1)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.5629e-18]], dtype=torch.float64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1 = execute_layer(X, W, b)\n",
    "L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we can pass this output into another layer, where we can have as many linear functions as we want, each of which takes two parameters and a bias term. And then apply a non-linearity to each of the linear function outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 1],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_3 = torch.tensor([6, 1]).view(2, 1)\n",
    "w_4 = torch.tensor([8, 9]).view(2, 1)\n",
    "\n",
    "b_2 = torch.tensor([5, 12])\n",
    "\n",
    "W_2 = torch.stack([w_3, w_4], 0).squeeze()\n",
    "W_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6701e-05, 2.2603e-06]], dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_layer(L1, W_2.double(), b.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We apply `.double` to make out type int tensors into `float64` tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the network, we would like to turn these different numbers into a prediction.  For example, for a classification function, like classifying a set of images into digits of 0-9.  To do that we can feed the output from our final layer into a softmax function.\n",
    "\n",
    "A softmax function exaggerates our positive numbers so that they are closer to one, and minimizes our smaller numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[neural networks intro](https://www.jeremyjordan.me/intro-to-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perhaps in lab make them write checks that inputs are of the correct dimensions\n",
    "* Add that in component about building the layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
